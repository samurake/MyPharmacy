from db_pymongo.mongoOrm import mongoORM
from requests.models import ReadTimeoutError
import config as cfg
from urllib import request
import requests
import bs4

"""
The idea is like this :
On https://www.anm.ro/nomenclator/medicamente the data we want is kept in 
~'table',class_="table table-striped table-condensed table-bordered"~.
There we have the data kept in td tags and all the data is kept under one 
of them in a button tag ~"button",class_="btn btn-primary btn-xs"~.
That button is the details button from the webpage and contains all the
data as values/attributes.
We are going to go on the webpage extract the data to see if any 
modification was made.
If we find any modification we go on every element and look for changes,
send a notification+log whatever was modified or added and update the DB
"""
import logging

log_error = print
log_info = print
log_critical_error = print

def _extract_data_from_button_element_(element:bs4.element) -> dict:
    """This function will get a element from the list generated by _extract_table_buttons_from_raw_html_string
    and return a dictionary containing the properties/value/attibutes required

    Args:
        element (bs4.element): button element retruned by _extract_table_buttons_from_raw_html_string in the list

    Returns:
        dict: a dicttionay containing the wanted data (cfg.hraw_to_hname_data.keys() and present as attributes of the element)
    """
    parsed_return = {}
    for keyword in cfg.hraw_to_hname_data.keys():
        #python will return a None if the value does not exist
        #this si made in such a way because the template might be changed and we can get only partial data or malformed data
        #for the purpose of standardization and not risking malformed that this is done in such a way
        kwdt = element.get(keyword)
        #kwdt = key-word-data
        if kwdt is not None: 
            parsed_return[keyword]= kwdt
        else: 
            return None
    return parsed_return
        
def _extract_table_buttons_from_raw_html_string (html:str):
    """This function will take a string, create a bs4 element and parse it.
    It will look for a specific element and then a specific subset of 
    subelements 
    Function used to retrieve the "Details" button and it's properties 

    Args:
        html (str): string containing the raw html

    Returns:
        bs4.elemet or None: Specific set of elements
    """
    try:
        soup = bs4.BeautifulSoup(html, 'html.parser')  
        table = soup.find('table',class_="table table-striped table-condensed table-bordered")
        detail_elements = table.find_all("button",class_="btn btn-primary btn-xs")
        return detail_elements
    except: 
        return None    
    
def get_url_data (url:str,params:str): 
    """This function will download the html of a website
    Function has a tuple as return, containing 3 elemernts:
        0 - None or the text/html of the webpage
        1 - None or status code
        2 - None or Exception if there is any error during the request

    Args:
        url (str): url to download data from
        params (str): Options, such as agent, proxy etc 
    """
    req_resp = None
    status_code = None
    err = None
    try:
        req = requests.get(url,params)
        req_resp = req.text
        status_code = req.status_code
    except Exception as e:
        err = str(e)
    finally:
        return(req_resp,status_code,err)        


def _parse_element_before_inserting(elem):
    def _extract_name_from_uri(uri):
        # split by / get last element
        return uri.split("/")[-1]

    """"data-linkrcp" : "Rezumat caracteristici produs",
    "data-linkpro" : "Prospect",
    "data-linkamb" : "Ambalaj","""
    elem["data-linkrcp-fname"] = _extract_name_from_uri(elem["data-linkrcp"])
    elem["data-linkpro-fname"] = _extract_name_from_uri(elem["data-linkpro"])
    elem["data-linkamb-fname"] = _extract_name_from_uri(elem["data-linkamb"])
    
    
def scrap_main():
    #Extract the information from the "main" page
    got_data = True
    page_index = 1
    # made like this so there is no need to add weird logic or download 2 webpages the same time 
    # also this will just work if elements change but not the query, same happens for the query too
    # if ever necessary it can be changed to look for  aspecific element and next uri available in the bar
    _total_requests_ = 0
    _successful_requests_ = 0
    _total_data_inserted_ = 0
    
    dbConn = mongoORM()
    while got_data:
        # do a request to the URL
        url_to_scrap = cfg.base_url+cfg.page_uri.format(page_index)
        req_data:tuple = get_url_data(url_to_scrap, None)
        print(url_to_scrap)
        # verify that the request was ok
        if req_data[1]==200:
            # proper request from website - > extract the data from the buttons
            buttons_detail = _extract_table_buttons_from_raw_html_string(req_data[0])
            if len (buttons_detail):
                # if any elements were able to be extracted - > extract the attributes we are interested in
                for elem in buttons_detail:
                    element_data = _extract_data_from_button_element_(elem)
                    _parse_element_before_inserting(element_data)
                    # print(element_data) 
                    dbConn.insertMedicament(element_data)
            else:
                print("No data To Extract on uri index: {ind}".format(ind=page_index))
                got_data = False
        else:
            log_error("Request for {} ! return of function {}".format(url_to_scrap,str(req_data)))
        page_index += 1
    else: 
        print("Script got a page that had no elements! either scrapping is done or some error occured")



scrap_main()


"""
To discuss:
    what's the fallback if we dont get a 200 on any of the requests.
    What do we do if no buttons/element are available
    
For db: im chosing 2 db:
    - > one containing the crapped data + 3 other fields (the filenames of the pdfs)
        - > maby one more for time inserted/scrapped
    - > one containing the pdf itself, name, a "flag" that will denote if it's still being used and a timestamp of the inserted data and the URL

Logic:
    This file will scrap the main wesbite for the plain text data and will either insert in the 2nd table the name 
    A second process will start the scrapping of the pdfs based on a "importance" level
        A importance level is a integer given to the pdfs 
            example: as "prospect" or "Rezumat caracteristici produs" are more important for the person rather than "Ambalaj" these 
                     will have a higher importance level and will be the first one downloaded
    Why is it made like this :
        In events that there are big changes to the databse a person might need information of the "pill" rathen than some of it's pdfs as these
        can still be accesed with their normal url
    
    These 2 processes will be independent as the donwloading of the pdfs will take a lot more than scrapping the primary data and some data can be
    available faster(aka data already present on the ANM interface)
    
    There will be a mongo db that will store the data that can only be accesible from the host machine, no outside Direct interraction
    A api that will require a token to acces the data and will support varios operations for the db (mainly accessing the database)
    If possible:
        adding ssl and https to the services
        tor routing for downloading of the PDFs
        
    Will require :
        jobs to start this file and the other process that downloads either daily at speccific hour or when this one finishes
"""